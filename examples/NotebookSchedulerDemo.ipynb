{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Scheduler HSNE Demo Notebook \n",
    "\n",
    "### Lucas Graybuck & Paul Mariz \n",
    "\n",
    "### 2021-05-02\n",
    "\n",
    "In this notebook, we'll use the Python HISE SDK to load a batch of .fcs files and run a HSNE dimension reduction. We'll discover midway through that the IDE instance we're on is too small, and that we need to schedule this notebook to run on a larger instance.\n",
    "\n",
    "To do this, we'll follow this overall flow:\n",
    "\n",
    "1. Get some files of interest from HISE Advanced search\n",
    "2. Read the data from the files and prepare it for HSNE\n",
    "3. Schedule the Notebook\n",
    "4. Wait for the scheduled job to complete\n",
    "5. Download the output\n",
    "6. Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.20.3\n",
      "Uninstalling numpy-1.20.3:\n",
      "  Successfully uninstalled numpy-1.20.3\n",
      "Collecting numpy==1.20.3\n",
      "  Using cached numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Successfully installed numpy-1.20.3\n",
      "Requirement already satisfied: cmake==3.20.2 in /opt/conda/lib/python3.7/site-packages (3.20.2)\n",
      "Requirement already satisfied: flowkit==0.6.1 in /opt/conda/lib/python3.7/site-packages (0.6.1)\n",
      "Requirement already satisfied: flowutils in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (0.9.3)\n",
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (0.12.0)\n",
      "Requirement already satisfied: flowio in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (0.9.11)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (0.11.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (1.1.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (3.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (1.20.3)\n",
      "Requirement already satisfied: bokeh in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (2.2.1)\n",
      "Requirement already satisfied: anytree in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (2.8.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (4.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from flowkit==0.6.1) (1.5.2)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.7/site-packages (from statsmodels->flowkit==0.6.1) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->flowkit==0.6.1) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->flowkit==0.6.1) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flowkit==0.6.1) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flowkit==0.6.1) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flowkit==0.6.1) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flowkit==0.6.1) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flowkit==0.6.1) (0.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from bokeh->flowkit==0.6.1) (3.7.4.2)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.7/site-packages (from bokeh->flowkit==0.6.1) (6.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.7/site-packages (from bokeh->flowkit==0.6.1) (20.4)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh->flowkit==0.6.1) (5.3.1)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /opt/conda/lib/python3.7/site-packages (from bokeh->flowkit==0.6.1) (2.11.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from anytree->flowkit==0.6.1) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh->flowkit==0.6.1) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "#I need a few extra libraries in order to run this analysis\n",
    "#You may also have to restart the notebook kernel in order to load the new version of numpy\n",
    "!pip uninstall -y numpy\n",
    "!pip install \"numpy == 1.20.3\"\n",
    "!pip install \"cmake == 3.20.2\"\n",
    "!pip install \"flowkit == 0.6.1\"\n",
    "!pip install nptsne\n",
    "!pip install pyreadr\n",
    "!pip install community\n",
    "!pip install python-louvain\n",    
    "import hisepy\n",
    "import flowkit\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "import networkx\n",
    "import nptsne\n",
    "import pyreadr\n",
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the SDK to read files from HISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [HISE UI](https://allenimmunology.org) I did a search for FCS files (I used the public \"flow QC fcs\" query and selected batch B001). That gave me this code snippet:\n",
    "```\n",
    "fres1 <- hise::readFiles(list(\"d00fd209-7c6c-4f39-a857-3c945ae23b82\", \"2f1ea469-587c-4815-bb6e-77462eaa0748\", \"d564eebd-241c-4025-8b20-cdf79325d9ad\", \"52297a4e-64c4-42b8-b963-cf3f402e1139\", \"886b5851-f1d7-47fb-a52c-10abec6311d3\", \"2cf0d3c1-a4e0-4921-b941-50465aabdff7\", \"155d9078-b357-42c6-baf7-488c98c6786b\", \"6e423656-f5f6-48fa-8862-b4e5191f70ee\", \"28192359-4a24-4677-8893-60eb1bd55464\", \"daf3dc9f-31b5-4876-86e6-2ad599a274f5\", \"b6e214ea-5fd6-4596-8ea4-7b6c47f1f61e\", \"38dc23f6-f199-4d5c-9f83-a300d14fbece\", \"6875bfa0-2466-4903-aa87-920ad1a94366\"))\n",
    "```\n",
    "That's R code, so I need to change it slightly to the python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcs_files = hisepy.read_files([\"d00fd209-7c6c-4f39-a857-3c945ae23b82\", \"2f1ea469-587c-4815-bb6e-77462eaa0748\", \"d564eebd-241c-4025-8b20-cdf79325d9ad\", \"52297a4e-64c4-42b8-b963-cf3f402e1139\", \"886b5851-f1d7-47fb-a52c-10abec6311d3\", \"2cf0d3c1-a4e0-4921-b941-50465aabdff7\", \"155d9078-b357-42c6-baf7-488c98c6786b\", \"6e423656-f5f6-48fa-8862-b4e5191f70ee\", \"28192359-4a24-4677-8893-60eb1bd55464\", \"daf3dc9f-31b5-4876-86e6-2ad599a274f5\", \"b6e214ea-5fd6-4596-8ea4-7b6c47f1f61e\", \"38dc23f6-f199-4d5c-9f83-a300d14fbece\", \"6875bfa0-2466-4903-aa87-920ad1a94366\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fcs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'CD3',\n",
       " 'CD45',\n",
       " 'CD15',\n",
       " 'CD45RA',\n",
       " 'CD14',\n",
       " 'CD8',\n",
       " 'CD11c',\n",
       " 'CD25',\n",
       " 'CD4',\n",
       " 'Live/Dead',\n",
       " 'CD16',\n",
       " 'CD123',\n",
       " 'CD127',\n",
       " 'IgD',\n",
       " 'CD304',\n",
       " 'CD141',\n",
       " 'CD11b',\n",
       " 'CD19',\n",
       " 'CD27',\n",
       " 'abTCR',\n",
       " 'CD34',\n",
       " 'CD197',\n",
       " 'CD38',\n",
       " 'CD56',\n",
       " 'HLA-DR',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = flowkit.Sample(fcs_files[0].path)\n",
    "test_sample.pns_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.02012023e+05, 8.91688438e+04, 1.44771969e+05, ...,\n",
       "        1.36947742e+03, 2.56013153e+02, 3.01377630e-02],\n",
       "       [1.19290570e+05, 1.08187844e+05, 1.45237297e+05, ...,\n",
       "        2.50700586e+04, 8.14550928e+03, 3.03957605e-02],\n",
       "       [1.24016266e+05, 1.07464906e+05, 1.46331109e+05, ...,\n",
       "        1.85670215e+03, 3.35393677e+02, 3.07395220e-02],\n",
       "       ...,\n",
       "       [6.96097500e+04, 6.05296680e+04, 1.34352266e+05, ...,\n",
       "        7.07714600e+02, 2.74119598e+02, 1.00024570e+02],\n",
       "       [1.28355367e+05, 1.14163805e+05, 1.38775109e+05, ...,\n",
       "        7.99269592e+02, 2.04366501e+02, 1.00024980e+02],\n",
       "       [5.93681406e+04, 5.58736875e+04, 1.21945148e+05, ...,\n",
       "        3.21582251e+03, 1.25084834e+04, 1.00025020e+02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_events = test_sample.get_raw_events()\n",
    "test_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert the fcs file data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.18 GiB for an array with shape (22, 7222425) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9c00e3759c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfcs_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcs_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpns_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfcs_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcs_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcs_events\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             )\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.18 GiB for an array with shape (22, 7222425) and data type float64"
     ]
    }
   ],
   "source": [
    "events = pandas.DataFrame()\n",
    "for fcs_file in fcs_files:\n",
    "    # Read sample\n",
    "    fcs_sample = flowkit.Sample(fcs_file.path)\n",
    "    # Get events\n",
    "    fcs_events = fcs_sample.get_raw_events()\n",
    "    # Convert to DataFrame for filtering columns\n",
    "    fcs_events = pandas.DataFrame(fcs_events)\n",
    "    fcs_events.columns = fcs_sample.pns_labels\n",
    "    fcs_events = fcs_events.iloc[:,10:]\n",
    "    events = pandas.concat([events, fcs_events], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no. I've got too much data for the instance I'm currently running, and I haven't even gotten to the HSNE dimension reduction yet. What can I do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Schedule This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to write the rest of the code that I want to execute on this data, and then schedule this notebook to run on a big instance. The first thing I will do is define some variables that name the output I expect my notebook to have, and set the project that I want those output files to belong to (I only need to do the second thing if I'm currently working on multiple projects in HISE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I expect this notebook to output two files:\n",
    "clustering_output = \"clustering.rds\"\n",
    "embedding_output = \"embedding.rds\"\n",
    "#it is VERY IMPORTANT to get the names of the output files correct when scheduling an instance\n",
    "\n",
    "output_project = \"cohorts\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to write a cell that has all the code I want to execute to produce those outputs. I have kind of a chicken-and-egg problem here: I don't actually want to run all this code right now. Instead I want to run it on the much bigger scheduled instance. So I'm going to write it all out and not execute the cell. But that's a little risky, because what if I accidentally make some typos or my code doesn't output the things I think it should? Here I don't have this problem because it's a sample notebook and I already tested the code. But what if that's not the case? Here are some things I could do:\n",
    "\n",
    " * Run the entire notebook on one .fcs file in order to test it, then change the code so it runs on all of them, then schedule it. \n",
    " * Execute the cell to make sure that it at least doesn't have any syntax errors, and then kill the kernel before it gets too far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to define this helper function before I use it\n",
    "def make_graph_from_transition_matrix(tmat):    \n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    for r_ind, rcol in enumerate(tmat):\n",
    "        for tup in rcol:\n",
    "            if not isinstance(tup, tuple):\n",
    "                continueexit()\n",
    "            row.append(r_ind)\n",
    "            col.append(tup[0])\n",
    "            data.append(tup[1])\n",
    "    \n",
    "    g = networkx.Graph()\n",
    "    g.add_weighted_edges_from(list(zip(row, col, data)))\n",
    "    return g\n",
    "\n",
    "#I'll add some print statements so I can debug this later if it fails\n",
    "print(\"Running HSNE\")\n",
    "hsne = nptsne.HSne(True)\n",
    "hsne.create_hsne(events.to_numpy(), 5)\n",
    "print(\"HSNE Completed\")\n",
    "hsne_scale = hsne.get_scale(4)\n",
    "#that helper function I defined above\n",
    "hsne_graph = make_graph_from_transition_matrix(hsne_scale.transition_matrix)   \n",
    "print(\"Running Louvain Partitioning\")\n",
    "clusters = community_louvain.best_partition(hsne_graph, resolution = 1)\n",
    "print(\"Partitioning Done - Saving clusters data frame as %s\" % (clustering_output))\n",
    "cluster_df = pandas.DataFrame(list(clusters.items()), columns = ['orig_cell','cluster_id'])\n",
    "cluster_df = cluster_df.sort_values('orig_cell')\n",
    "cluster_df = cluster_df.reset_index()\n",
    "cluster_df['cell_idx'] = list(hsne_scale.landmark_orig_indexes)\n",
    "#write the clustering dataframe to an output file\n",
    "pyreadr.write_rds(clustering_output, cluster_df)\n",
    "print(\"Running embedding\")\n",
    "model = nptsne.hsne_analysis.Analysis(hsne, nptsne.hsne_analysis.EmbedderType.CPU)\n",
    "for i in range(2000):\n",
    "    model.do_iteration()\n",
    "nptsne_embedding = model.embedding\n",
    "print(\"Embedding done -- Saving embedding data frame as %s\" % (embedding_output))\n",
    "nptsne_df = pandas.DataFrame({'x' : [val[0] for val in nptsne_embedding], 'y' : [val[1] for val in nptsne_embedding]})\n",
    "nptsne_df['cell_idx'] = hsne_scale.landmark_orig_indexes\n",
    "nptsne_df = nptsne_df.sort_values('cell_idx')\n",
    "nptsne_df = nptsne_df.reset_index()\n",
    "nptsne_df['cluster_id'] = cluster_df['cluster_id'].astype('category')\n",
    "#write the embedding output\n",
    "pyreadr.write_rds(embedding_output, nptsne_df)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now I'm ready to schedule. I just pass the output files I expect as an array to the scheduler function. When this scheduled notebook runs later, it will execute the entire notebook, cell by cell. The only difference is that it will ignore the schedule_notebook function, because we do not want the scheduled notebook to schedule another notebook (and so on, forever. That would, like, be bad?)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to schedule notebook /home/jupyter/examples/NotebookSchedulerDemo.ipynb for run on a large instance.\n",
      "I will run all the cells in the notebook, only skipping this schedule function.\n",
      "I expect this notebook to produce the following output files:\n",
      "\tclustering.rds\n",
      "\tembedding.rds\n",
      "I will copy those files back to HISE where they will be available for later download into this or another IDE instance.\n",
      "OK? (y/n) "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduling...\n",
      "Scheduled.\n"
     ]
    }
   ],
   "source": [
    "job = hisepy.schedule_notebook([clustering_output, embedding_output], project = output_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.check_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.is_completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Wait for Scheduled Job To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can periodically run the either of the cells above (job.check_status() or job.is_completed() to check on the status of my job). I can also visit https://allenimmunology/#/notebook-jobs to see the status of my job. I can also shut this notebook down now and resume it later by clicking the \"Clone IDE\" button next to my completed job on the notebook-jobs page.\n",
    "\n",
    "\n",
    "&lt;six hours go by&gt;\n",
    "\n",
    "...finally..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Download The Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = job.download_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&lt;insert your own profitable code here&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
